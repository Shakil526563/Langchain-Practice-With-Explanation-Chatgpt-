{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNO6WQP9+vWbEswyXRO7DXi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GGT8m5ozmvtW"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["рждрзБржорж┐ ржЬрж╛ржирждрзЗ ржЪрж╛ржУ ржХржЦржи ржПржмржВ ржХрзЗржи LangChain-ржП ржирж┐ржЪрзЗрж░ **memory classes** ржмрзНржпржмрж╣рж╛рж░ ржХрж░рж╛ рж╣рзЯ тАФ\n","\n","* `ConversationBufferMemory`\n","* `ConversationSummaryMemory`\n","* `ConversationTokenBufferMemory`\n","\n","ржПржЦрж╛ржирзЗ **Bangla + English** ржжрзБржЯрзЛ ржнрж╛рж╖рж╛рзЯ ржмрж┐рж╕рзНрждрж╛рж░рж┐ржд ржмрзНржпрж╛ржЦрзНржпрж╛ ржжрж┐рзЯрзЗ ржжрж┐рж▓рж╛ржо:\n","\n","---\n","\n","## тЬЕ `from langchain.memory import ConversationBufferMemory`\n","\n","ЁЯФ╣ **ЁЯЗзЁЯЗй ржХржЦржи ржУ ржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржм**:\n","ржпржЦржи рждрзБржорж┐ ржЪрж╛ржУ ржкрзБрж░рзЛ conversation history (user ржУ AI ржПрж░ рж╕ржм ржорзЗрж╕рзЗржЬ) **ржПржХржЯрж╛ржирж╛ рж╕ржВрж░ржХрзНрж╖рж┐ржд ржерж╛ржХрзБржХ**, ржХрзЛржирзЛ рж╕ржВржХрзНрж╖рзЗржк ржЫрж╛ржбрж╝рж╛ржЗред\n","\n","ЁЯФ╣ **ЁЯЗмЁЯЗз When & Why to Use**:\n","Use this to keep **the full chat history** (verbatim), so the model can reference earlier messages **as-is**.\n","\n","ЁЯза **Use case**:\n","\n","* Personal assistants\n","* Chatbots where **context is important**\n","* Full memory recall\n","\n","ЁЯУМ **Example**:\n","\n","```python\n","from langchain.memory import ConversationBufferMemory\n","\n","memory = ConversationBufferMemory()\n","```\n","\n","---\n","\n","## тЬЕ `from langchain.memory import ConversationSummaryMemory`\n","\n","ЁЯФ╣ **ЁЯЗзЁЯЗй ржХржЦржи ржУ ржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржм**:\n","ржпржЦржи ржкрзБрж░рзЗрж╛ ржорзЗрж╕рзЗржЬ ржирж╛ рж░рзЗржЦрзЗ, **рж╕ржВржХрзНрж╖рзЗржкрзЗ conversation summary** рж░рж╛ржЦрждрзЗ ржЪрж╛ржУ тАФ ржпрж╛рждрзЗ ржорзЗржорж░рж┐ ржЫрзЛржЯ рж╣рзЯ ржПржмржВ рждржерзНржп ржзрж░рзЗ рж░рж╛ржЦрж╛ ржпрж╛рзЯред\n","\n","ЁЯФ╣ **ЁЯЗмЁЯЗз When & Why to Use**:\n","Use when you want to keep **summarized memory**, not the entire conversation, to **save tokens** and improve performance.\n","\n","ЁЯза **Use case**:\n","\n","* Long-term conversational bots\n","* Reduced memory cost\n","* When full message history is too large\n","\n","ЁЯУМ **Example**:\n","\n","```python\n","from langchain.memory import ConversationSummaryMemory\n","from langchain.chat_models import ChatOpenAI\n","\n","memory = ConversationSummaryMemory(llm=ChatOpenAI())\n","```\n","\n","---\n","\n","## тЬЕ `from langchain.memory import ConversationTokenBufferMemory`\n","\n","ЁЯФ╣ **ЁЯЗзЁЯЗй ржХржЦржи ржУ ржХрзЗржи ржмрзНржпржмрж╣рж╛рж░ ржХрж░ржм**:\n","ржпржЦржи рждрзБржорж┐ ржирж┐рж░рзНржжрж┐рж╖рзНржЯ **рж╕ржВржЦрзНржпржХ ржЯрзЛржХрзЗржи ржкрж░рзНржпржирзНржд ржорзЗржорж░рж┐ рж░рж╛ржЦрждрзЗ ржЪрж╛ржУ**, ржпрж╛рждрзЗ context window overflow ржирж╛ рж╣рзЯред\n","\n","ЁЯФ╣ **ЁЯЗмЁЯЗз When & Why to Use**:\n","Use when you want to **limit the memory** to a certain number of tokens. Great for models with small context window.\n","\n","ЁЯза **Use case**:\n","\n","* Token-efficient bots\n","* Preventing long input errors (e.g., GPT-3.5 has \\~4k tokens limit)\n","\n","ЁЯУМ **Example**:\n","\n","```python\n","from langchain.memory import ConversationTokenBufferMemory\n","\n","memory = ConversationTokenBufferMemory(max_token_limit=500)\n","```\n","\n","---\n","\n","## ЁЯза Summary Table\n","\n","| Memory Type                     | ЁЯЗзЁЯЗй ржмрзНржпржмрж╣рж╛рж░ ржХрж░рзБржи ржпржЦржи...          | ЁЯЗмЁЯЗз Use When...                | ЁЯза Benefit                     |\n","| ------------------------------- | --------------------------------- | ------------------------------- | ------------------------------ |\n","| `ConversationBufferMemory`      | рж╕ржорзНржкрзВрж░рзНржг ржорзЗрж╕рзЗржЬ ржЗрждрж┐рж╣рж╛рж╕ рж░рж╛ржЦрждрзЗ ржЪрж╛ржи   | Full chat history needed        | Full recall of conversation    |\n","| `ConversationSummaryMemory`     | рж╕ржВржХрзНрж╖рж┐ржкрзНржд рж╕рж╛рж░рж╛ржВрж╢ рж░рж╛ржЦрж▓рзЗ ржнрж╛рж▓рзЛ рж╣рзЯ    | Summary of past chats is enough | Saves token & context space    |\n","| `ConversationTokenBufferMemory` | ржирж┐рж░рзНржжрж┐рж╖рзНржЯ ржЯрзЛржХрзЗржи ржкрж░рзНржпржирзНржд рж░рж╛ржЦрждрзЗ ржЪрж╛ржи | You want token-limited memory   | Avoids context length overflow |\n","\n","---\n","\n","## тЬЕ BONUS: Use in a Chain\n","\n","```python\n","from langchain.chains import ConversationChain\n","from langchain.chat_models import ChatOpenAI\n","from langchain.memory import ConversationSummaryMemory\n","\n","llm = ChatOpenAI(model_name=\"gpt-4\")\n","\n","memory = ConversationSummaryMemory(llm=llm)\n","\n","chain = ConversationChain(llm=llm, memory=memory, verbose=True)\n","\n","chain.predict(input=\"Hi! I'm planning a vacation to Nepal.\")\n","```\n","\n","---\n","\n","ЁЯФЪ **рж╢рзЗрж╖ ржХржерж╛ (Final Note)**\n","ржПржЗ ржорзЗржорж░рж┐ рж╕рж┐рж╕рзНржЯрзЗржо ржЧрзБрж▓рзЛ ChatGPT/LLM ржХрзЗ ржЖрж░ржУ рж╕рзНржорж╛рж░рзНржЯ ржХрж░рзЗ рждрзЛрж▓рзЗред рждрзБржорж┐ ржЪрж╛ржЗрж▓рзЗ ржЖржорж┐ рждрзЛржорж╛рж░ ржЬржирзНржп ржПржХржЯрж╛ ржЫрзЛржЯ **memory-based chatbot project** ржмрж╛ржирж┐рзЯрзЗ ржжрж┐рждрзЗ ржкрж╛рж░рж┐ред\n","\n","**Do you want a full LangChain chatbot project using one of these memory types?**\n"],"metadata":{"id":"pIK9m0lzoeiL"}}]}